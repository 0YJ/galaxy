<?xml version="1.0"?>
<job_conf>
    <plugins workers="4">
        <!-- "workers" is the number of threads for the runner's work queue.
             The default from <plugins> is used if not defined for a <plugin>.
          -->
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="pbs" type="runner" load="galaxy.jobs.runners.pbs:PBSJobRunner" workers="2"/>
        <plugin id="drmaa" type="runner" load="galaxy.jobs.runners.drmaa:DRMAAJobRunner">
            <!-- Different DRMs handle successfully completed jobs differently,
                 these options can be changed to handle such differences and
                 are explained in detail on the Galaxy wiki. Defaults are shown -->
            <param id="invalidjobexception_state">ok</param>
            <param id="invalidjobexception_retries">0</param>
            <param id="internalexception_state">ok</param>
            <param id="internalexception_retries">0</param>
        </plugin>
        <plugin id="sge" type="runner" load="galaxy.jobs.runners.drmaa:DRMAAJobRunner">
            <!-- Override the $DRMAA_LIBRARY_PATH environment variable -->
            <param id="drmaa_library_path">/sge/lib/libdrmaa.so</param>
        </plugin>
        <plugin id="cli" type="runner" load="galaxy.jobs.runners.cli:ShellJobRunner" />
        <plugin id="condor" type="runner" load="galaxy.jobs.runners.condor:CondorJobRunner" />
        <plugin id="slurm" type="runner" load="galaxy.jobs.runners.slurm:SlurmJobRunner" />
        <plugin id="dynamic" type="runner">
            <!-- The dynamic runner is not a real job running plugin and is
                 always loaded, so it does not need to be explicitly stated in
                 <plugins>. However, if you wish to change the base module
                 containing your dynamic rules, you can do so.

                 The `load` attribute is not required (and ignored if
                 included).
            -->
            <param id="rules_module">galaxy.jobs.rules</param>
        </plugin>
        <!-- Pulsar runners (see more at https://pulsar.readthedocs.org) -->
        <plugin id="pulsar_rest" type="runner" load="galaxy.jobs.runners.pulsar:PulsarRESTJobRunner">
          <!-- Allow optimized HTTP calls with libcurl (defaults to urllib) -->
          <!-- <param id="transport">curl</param> -->

          <!-- *Experimental Caching*: Next parameter enables caching.
                Likely will not work with newer features such as MQ support.

                If this is enabled be sure to specify a `file_cache_dir` in
                the remote Pulsar's servers main configuration file.
          -->
          <!-- <param id="cache">True</param> -->
        </plugin>
        <plugin id="pulsar_mq" type="runner" load="galaxy.jobs.runners.pulsar:PulsarMQJobRunner">
          <!-- AMQP URL to connect to. -->
          <param id="amqp_url">amqp://guest:guest@localhost:5672//</param>
          <!-- URL remote Pulsar apps should transfer files to this Galaxy
               instance to/from. This can be unspecified/empty if
               galaxy_infrastructure_url is set in galaxy.ini.
          -->
          <param id="galaxy_url">http://localhost:8080</param>
          <!-- AMQP does not guarantee that a published message is received by
               the AMQP server, so Galaxy/Pulsar can request that the consumer
               acknowledge messages and will resend them if acknowledgement is
               not received after a configurable timeout.  -->
          <!-- <param id="amqp_acknowledge">False</param> -->
          <!-- Galaxy reuses Pulsar's persistence_directory parameter (via the
               Pulsar client lib) to store a record of received
               acknowledgements, and to keep track of messages which have not
               been acknowledged. -->
          <!-- <param id="persistence_directory">/path/to/dir</param> -->
          <!-- Number of seconds to wait for an acknowledgement before
               republishing a message. -->
          <!-- <param id="amqp_republish_time">30</param> -->
          <!-- Pulsar job manager to communicate with (see Pulsar
               docs for information on job managers). -->
          <!-- <param id="manager">_default_</param> -->
          <!-- The AMQP client can provide an SSL client certificate (e.g. for
               validation), the following options configure that certificate
               (see for reference:
                 http://kombu.readthedocs.org/en/latest/reference/kombu.connection.html
               ). If you simply want to use SSL but not use/validate a client
               cert, just use the ?ssl=1 query on the amqp URL instead. -->
          <!-- <param id="amqp_connect_ssl_ca_certs">/path/to/cacert.pem</param> -->
          <!-- <param id="amqp_connect_ssl_keyfile">/path/to/key.pem</param> -->
          <!-- <param id="amqp_connect_ssl_certfile">/path/to/cert.pem</param> -->
          <!-- <param id="amqp_connect_ssl_cert_reqs">cert_required</param> -->
          <!-- By default, the AMQP consumer uses a nonblocking connection with
               a 0.2 second timeout. In testing, this works fine for
               unencrypted AMQP connections, but with SSL it will cause the
               client to reconnect to the server after each timeout. Set to a
               higher value (in seconds) (or `None` to use blocking connections). -->
          <!-- <param id="amqp_consumer_timeout">None</param> -->
        </plugin>
        <plugin id="pulsar_legacy" type="runner" load="galaxy.jobs.runners.pulsar:PulsarLegacyJobRunner" shell="none">
          <!-- Pulsar job runner with default parameters matching those
               of old LWR job runner. If your Pulsar server is running on a
               Windows machine for instance this runner should still be used.

               These destinations still needs to target a Pulsar server,
               older LWR plugins and destinations still work in Galaxy can
               target LWR servers, but this support should be considered
               deprecated and will disappear with a future release of Galaxy.
          -->
        </plugin>
        <plugin id="pulsar_embedded" type="runner" load="galaxy.jobs.runners.pulsar:PulsarEmbeddedJobRunner">
          <!-- The embedded Pulsar runner starts a Pulsar app
               internal to Galaxy and communicates it directly.
               This maybe be useful for instance when Pulsar
               staging is important but a Pulsar server is
               unneeded (for instance if compute servers cannot
               mount Galaxy's files but Galaxy can mount a
               scratch directory available on compute). -->
          <!-- Specify a complete description of the Pulsar app
               to create. If this configuration defines more than
               one manager - you can specify the manager name 
               using the "manager" destination parameter. For more
               information on configuring a Pulsar app see:

               https://github.com/galaxyproject/pulsar/blob/master/app.yml.sample
               http://pulsar.readthedocs.org/en/latest/configure.html
          -->
          <!-- <param id="pulsar_config">path/to/pulsar/app.yml</param> -->
        </plugin>
        <plugin id="k8s" type="runner" load="galaxy.jobs.runners.kubernetes:KubernetesJobRunner">
            <!-- The Kubernetes (k8s) plugin allows to send jobs to a k8s cluster which shares filesystem with Galaxy.

                 This requires installing pykube. Install pykube by activating Galaxy's virtual
                 and then executing the following pip command:

                 pip install pykube==0.15.0

                 The shared file system needs to be exposed to k8s through a Persistent Volume (rw) and a Persistent
                 Volume Claim. An example of a Persistent Volume could be, in yaml (access modes, reclaim policy and
                 path are relevant) (persistent_volume.yaml):

                    kind: PersistentVolume
                    apiVersion: v1
                    metadata:
                      name: pv-galaxy-nfs
                      labels:
                        type: nfs
                    spec:
                      capacity:
                        storage: 10Gi
                      accessModes:
                        - ReadWriteMany
                      persistentVolumeReclaimPolicy: Retained
                      nfs:
                        path: /scratch1/galaxy_data
                        server: 192.168.64.1

                 The path (nfs:path: in the example) set needs to be a parent directory of the directories used for
                 variables “file_path” and “new_file_path” on the galaxy.ini files. Clearly, for this particular example
                 to work, there needs to be a NFS server serving that directory on that ip. Please make sure that you
                 use reasonable storage size for your set up (possibly larger that the 10Gi written).
                 An example of the volume claim should be (this needs to be followed more closely) (pv_claim.yaml):

                    kind: PersistentVolumeClaim
                    apiVersion: v1
                    metadata:
                      name: galaxy-pvc
                    spec:
                      accessModes:
                        - ReadWriteMany
                      volumeName: pv-galaxy-nfs
                      resources:
                        requests:
                          storage: 2Gi

                 The volume claim needs to reference the name of the volume in spec:volumeName. The name of the claim
                 (metadat:name) is referenced in the plugin definition (see below), through param
                 "k8s_persistent_volume_claim_name". These two k8s object need to be created before galaxy can use them:

                 kubectl create -f <path/to/persistent_volume.yaml>
                 kubectl create -f <path/to/pv_claim.yaml>

                 pointing of course to the same Kubernetes cluster that you intend to use.
            -->

            <param id="k8s_config_path">/path/to/kubeconfig</param>
            <!-- This is the path to the kube config file, which is normally on ~/.kube/config, but that will depend on
                 your installation. This is the file that tells the plugin where the k8s cluster is, access credentials,
                 etc. This parameter is not necessary and ignored if k8s_use_service_account is set to True -->

            <!-- <param id="k8s_pull_policy">Default</param> -->
            <!-- Sets the pull policy to be used for all containers invoked for jobs by the Kubernetes cluster. Can take
                 any value among the possible Kubernetes container image pull policies: Always, IfNotPresent or Never
                 (respecting capitalization). Any other value, such as "Default", will not change the setting on k8s and
                 leave for the containers to run with the default pull policy specified at the cluster (this is normally
                 IfNotPresent). Container images using latest are by default always pulled, so you need to use defined
                 versions for offline cases to work (and images need to be previously pulled). -->

            <!-- <param id="k8s_use_service_account">false</param> -->
            <!-- For use when Kubernetes should be accessed from inside a Pod running Galaxy (that is,
                 galaxy is running inside Kubernetes). If this variable is True, then the previous k8s_config_path is
                 not required and actually ignored. It is not necessary to set this value if not setting it to true -->

            <!-- <param id="k8s_job_api_version">batch/v1</param> -->
            <!-- Version of the Kubernetes Job API object to use, the default one batch/v1 should be supported from
                 Kubernetes 1.2 and on. Changing this a much newer version in the future might require changes to the
                 plugin runner code. Value extensions/v1beta1 is also supported for pre 1.2 legacy installations.
                 -->
            <param id="k8s_persistent_volume_claim_name">galaxy_pvc</param>
            <!-- The name of the Persisten Volume Claim (PVC) to be used, details above, needs to match the PVC's
                 metadata:name -->

            <param id="k8s_persistent_volume_claim_mount_path">/scratch1/galaxy_data</param>
            <!-- The mount path needs to be parent directory of the "file_path" and "new_file_path" paths
                 set in universe_wsgi.ini (or equivalent general galaxy config file). This is the mount path of the
                 PVC within the docker container that will be actually running the tool -->

            <!-- <param id="k8s_namespace">default</param> -->
            <!-- The namespace to be used on the Kubernetes cluster, if different from default, this needs to be set
                 accordingly in the PV and PVC detailed above -->

            <!-- <param id="k8s_pod_retrials">4</param> -->
            <!-- Allows pods to retry up to this number of times, before marking the galaxy job failed. k8s is a state
                 setter essentially, so by default it will try to take a job submitted to successful completion. A job
                 submits pods, until the number of successes (1 in this use case) is achieved, assuming that whatever is
                 making the pods fail will be fixed (such as a stale disk or a dead node that it is being restarted).
                 This option sets a limit of retrials, so that after that number of failed pods, the job is re-scaled to
                 zero (no execution) and the stderr/stdout of the k8s job is reported in galaxy (and the galaxy job set
                 to failed) -->

            <!-- <param id="k8s_supplemental_group_id">0</param> -->
            <!-- <param id="k8s_fs_group_id">0</param> -->
            <!-- If mounting an NFS / GlusterFS or other shared file system which is administered to ONLY provide access
                 to a DEFINED user/group, these variables set the group id that Pods need to use to be able to read and
                 write from that mount. If left to zero or deleted, these parameters are neglected. Integer values
                 above zero trigger the addition of a security context on each Pod created to dispatch jobs:

                 securityContext:
                    supplementalGroups: [value-set-goes-here]
                    fsGroup: fs-group-integer-value

                 inside the Pod spec, applicable for all containers on the pod. Just one of them set to >0 will generate
                 the security context.

                 Using this requires that the Kubernetes cluster is not running the admission controller
                 "SecurityContextDeny". To check this, look at the admission-control= variable setup for the
                 api-server pod definition (normally in /etc/kubernetes/manifests/kube-apiserver.manifest), it shouldn't
                 have the mentioned admission controller. Pods with the securityContext set will fail if such admission
                 controller is present. Removing that admission controller from the manifest should provoke kubelet
                 to restart the api-server pod running on that machine (although this might vary on your Kubernetes
                 installation).

                 For more information see point 21.2.4.1 Group IDs on:
                 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.4/html/
                 installation_and_configuration/configuring-persistent-storage

                 Different storage strategies might or might not require supplemental groups or fs groups, one is not
                 a requirement of the other.

                 -->
        </plugin>
        <plugin id="godocker" type="runner" load="galaxy.jobs.runners.godocker:GodockerJobRunner">
            <!-- Go-Docker is a batch computing/cluster management tool using Docker
                 See https://bitbucket.org/osallou/go-docker for more details.  -->
            <!--  REST based runner , submits jobs to godocker -->
            <param id="godocker_master">GODOCKER_URL</param>
            <!-- Specify the instance of GoDocker -->
            <param id="user">USERNAME</param>
            <!-- GoDocker username -->
            <param id="key">APIKEY</param>
            <!-- GoDocker API key -->
            <param id="godocker_project">galaxy</param>
            <!-- Specify the project present in the GoDocker setup -->
        </plugin>
        <plugin id="chronos" type="runner" load="galaxy.jobs.runners.chronos:ChronosJobRunner">
            <!-- Chronos is a framework for the Apache Mesos software; a software which manages
                 computer clusters. Specifically, Chronos runs of top of Mesos and it's used
                 for job orchestration.

                 This runner requires a shared file system where the directories of
                 `job_working_directory`, `file_path` and `new_file_path` settings defined on
                 the `galaxy.ini` file are shared amongst the Mesos agents (i.e. nodes which
                 actually run the jobs).
            -->
            <param id="chronos">`chronos_host`</param>
            <!-- Hostname which runs Chronos instance. -->
            <param id="owner">foo@bar.com</param>
            <!-- The email address of the person responsible for the job. -->
            <param id="username">username</param>
            <!-- Username to access Mesos cluster. -->
            <param id="password">password</param>
            <!-- Password to access Mesos cluster. -->
            <param id="insecure">true</param>
            <!-- True to communicate with Chronos over HTTPS; false otherwise-->
        </plugin>
        <!-- Additionally any plugin or destination (below) may define an "enabled" parameter that should
             evaluate to True or False. When setup using
                <param id="enabled" from_environ="<VAR>">True|False</param>
             plugins and destinations can be conditionally loaded using environment variables.
             Setting the param body above to True or False is required and specifies the default
             used by Galaxy is no environment variable of the specified name is found.
        -->
    </plugins>
    <handlers default="handlers">
        <!-- Additional job handlers - the id should match the name of a
             [server:<id>] in galaxy.ini.
         -->
        <handler id="handler0" tags="handlers"/>
        <handler id="handler1" tags="handlers"/>
        <!-- Handlers will load all plugins defined in the <plugins> collection
             above by default, but can be limited to a subset using <plugin>
             tags. This is useful for heterogenous environments where the DRMAA
             plugin would need to be loaded more than once with different
             configs.
         -->
        <handler id="sge_handler">
            <plugin id="sge"/>
        </handler>
        <handler id="special_handler0" tags="special_handlers"/>
        <handler id="special_handler1" tags="special_handlers"/>
        <handler id="trackster_handler"/>
    </handlers>



    
    <destinations default="mid_specs">
        <destination id="slow_machine" runner="local"/>
        <destination id="mid_specs" runner="local"/>
        <destination id="so_specs" runner="local"/>
        <destination id="lg_nexus_4" runner="local"/>
        <destination id="test1" runner="local"/>
        <destination id="test2" runner="local"/>
        <destination id="test3" runner="local"/>
        <destination id="test4" runner="local"/>
        <destination id="5_year_old_acer" runner="local"/>
        <destination id="mid_machine" runner="local"/>
        <destination id="prime_minister's_computer" runner="local"/>

        <!-- for galaxy unit test -->
        <destination id="Destination1_high" runner="local"/>
        <destination id="Destination1_med" runner="local"/>
        <destination id="Destination1_low" runner="local"/>
        <destination id="Destination1" runner="local"/>
        <destination id="Destination2_high" runner="local"/>
        <destination id="Destination2_med" runner="local"/>
        <destination id="Destination2_low" runner="local"/>
        <destination id="Destination2" runner="local"/>
        <destination id="Destination3_high" runner="local"/>
        <destination id="Destination3_med" runner="local"/>
        <destination id="Destination3" runner="local"/>
        <destination id="Destination3_low" runner="local"/>
        <destination id="Destination4_high" runner="local"/>
        <destination id="Destination4_med" runner="local"/>
        <destination id="Destination4_low" runner="local"/>
        <destination id="Destination4" runner="local"/>
        <destination id="Destination5_high" runner="local"/>
        <destination id="Destination5_med" runner="local"/>
        <destination id="Destination5_low" runner="local"/>
        <destination id="Destination5" runner="local"/>
        <destination id="Destination6_high" runner="local"/>
        <destination id="Destination6_med" runner="local"/>
        <destination id="Destination6_low" runner="local"/>
        <destination id="Destination6" runner="local"/>
        <destination id="waffles_default_high" runner="local"/>
        <destination id="waffles_default_med" runner="local"/>
        <destination id="waffles_default_low" runner="local"/>
        <destination id="waffles_high" runner="local"/>
        <destination id="waffles_low" runner="local"/>
        <destination id="waffles_low_4" runner="local"/>
        <destination id="cluster_high_32" runner="local"/>
        <destination id="cluster_low_4" runner="local"/>
        <destination id="cluster_med_4" runner="local"/>
        <destination id="waffles_default" runner="local"/>
        <destination id="DestinationF" runner="local"/>
        <destination id="special_cluster" runner="local"/>
        <destination id="lame_cluster" runner="local"/>
        <destination id="even_lamer_cluster" runner="local"/>
        <destination id="things" runner="local"/>
    </destinations>



    
    <resources default="default">
      <!-- Group different parameters defined in job_resource_params_conf.xml
           together and assign these groups ids. Tool section below can map
           tools to different groups. This is experimental functionality!
      -->
      <group id="default"></group>
      <group id="memoryonly">memory</group>
      <group id="all">processors,memory,time,project</group>
    </resources>
    <tools>
        <!-- Tools can be configured to use specific destinations or handlers,
             identified by either the "id" or "tags" attribute.  If assigned to
             a tag, a handler or destination that matches that tag will be
             chosen at random.
         -->
        <tool id="foo" handler="trackster_handler">
            <param id="source">trackster</param>
        </tool>
        <tool id="bar" destination="dynamic"/>
        <!-- Next example defines resource group to insert into tool interface
             and pass to dynamic destination (as resource_params argument). -->
        <tool id="longbar" destination="dynamic" resources="all" />
        <tool id="baz" handler="special_handlers" destination="bigmem"/>

        <!-- Finally for Kubernetes runner, the following connects a particular tool to be executed with
             the container of choice in Kubernetes.
        -->
        <tool id="my-tool" destination="my-tool-container"/>
    </tools>
    <limits>
        <!-- Certain limits can be defined. The 'concurrent_jobs' limits all
             control the number of jobs that can be "active" at a time, that
             is, dispatched to a runner and in the 'queued' or 'running'
             states.

             A race condition exists that will allow destination_* concurrency
             limits to be surpassed when multiple handlers are allowed to
             handle jobs for the same destination. To prevent this, assign all
             jobs for a specific destination to a single handler.
        -->
        <!-- registered_user_concurrent_jobs:
                Limit on the number of jobs a user with a registered Galaxy
                account can have active across all destinations.
        -->
        <limit type="registered_user_concurrent_jobs">2</limit>
        <!-- anonymous_user_concurrent_jobs:
                Likewise, but for unregistered/anonymous users.
        -->
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <!-- destination_user_concurrent_jobs:
                The number of jobs a user can have active in the specified
                destination, or across all destinations identified by the
                specified tag. (formerly: concurrent_jobs)
        -->
        <limit type="destination_user_concurrent_jobs" id="local">1</limit>
        <limit type="destination_user_concurrent_jobs" tag="mycluster">2</limit>
        <limit type="destination_user_concurrent_jobs" tag="longjobs">1</limit>
        <!-- destination_total_concurrent_jobs:
                The number of jobs that can be active in the specified
                destination (or across all destinations identified by the
                specified tag) by any/all users.
        -->
        <limit type="destination_total_concurrent_jobs" id="local">16</limit>
        <limit type="destination_total_concurrent_jobs" tag="longjobs">100</limit>
        <!-- walltime:
                Amount of time a job can run (in any destination) before it
                will be terminated by Galaxy.
         -->
        <limit type="walltime">24:00:00</limit>
	<!-- total_walltime:
		Total walltime that jobs may not exceed during a set period.
		If total walltime of finished jobs exceeds this value, any
		new jobs are paused.  `window` is a number in days,
		representing the period.
         -->
        <limit type="total_walltime" window="30">24:00:00</limit>
        <!-- output_size:
                Size that any defined tool output can grow to before the job
                will be terminated. This does not include temporary files
                created by the job. Format is flexible, e.g.:
                '10GB' = '10g' = '10240 Mb' = '10737418240'
        -->
        <limit type="output_size">10GB</limit>
    </limits>
    <macros>
        <xml name="foohost_destination" tokens="id,walltime,ncpus">
            <destination id="@ID@" runner="cli">
                <param id="shell_plugin">SecureShell</param>
                <param id="job_plugin">Torque</param>
                <param id="shell_username">galaxy</param>
                <param id="shell_hostname">foohost_destination.example.org</param>
                <param id="job_Resource_List">walltime=@WALLTIME@,ncpus=@NCPUS@</param>
            </destination>
        </xml>
    </macros>
</job_conf>
